---
title: "Pre-Course Assignment"
description: "This small portfolio was created as a pre-course, which means before any classes in R at LBS where taken"
slug: pre_course
image: desperate.jpg
keywords: ""
categories: 
    - ""
    - ""
date: 2022-08-31
draft: false
---

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(gapminder)  # gapminder dataset
library(here)
library(janitor)
```

The goal is to test your software installation, to demonstrate
competency in Markdown, and in the basics of `ggplot`.

# Task 2: `gapminder` country comparison

You have seen the `gapminder` dataset that has data on life expectancy,
population, and GDP per capita for 142 countries from 1952 to 2007. To
get a glimpse of the dataframe, namely to see the variable names,
variable types, etc., we use the `glimpse` function. We also want to
have a look at the first 20 rows of data.

```{r}
glimpse(gapminder)

head(gapminder, 20) # look at the first 20 rows of the dataframe

```

Your task is to produce two graphs of how life expectancy has changed
over the years for the `country` and the `continent` you come from.

I have created the `country_data` and `continent_data` with the code
below.

```{r}
country_data <- gapminder %>% 
            filter(country == "Italy") # just choosing Italy, as this is where I come from

continent_data <- gapminder %>% 
            filter(continent == "Europe")
```

First, create a plot of life expectancy over time for the single country
you chose. Map `year` on the x-axis, and `lifeExp` on the y-axis. You
should also use `geom_point()` to see the actual data points and
`geom_smooth(se = FALSE)` to plot the underlying trendlines. You need to
remove the comments **\#** from the lines below for your code to run.

```{r, lifeExp_one_country}
plot1 <- ggplot(data = country_data, mapping = aes(x = year, y = lifeExp))+
  geom_point() +
  geom_smooth(se = FALSE)+
  NULL 

```

Next we need to add a title. Create a new plot, or extend plot1, using
the `labs()` function to add an informative title to the plot.

```{r, lifeExp_one_country_with_label}
plot1 <- plot1 +
  labs(title = "Life Expectancy in Italy from 1952 to 2007",
      x = "Year",
      y = "Life Expectancy") +
  NULL


plot1
```

Secondly, produce a plot for all countries in the *continent* you come
from. (Hint: map the `country` variable to the colour aesthetic. You
also want to map `country` to the `group` aesthetic, so all points for
each country are grouped together).

```{r lifeExp_one_continent}
 ggplot(data = continent_data, mapping = aes(x =  year, y = lifeExp  , colour = country, group = country))+
  geom_point() + 
  geom_smooth(se = FALSE) +
  NULL +
  ggtitle("Life Expectancy in Europe from 1952 to 2007")
```

Finally, using the original `gapminder` data, produce a life expectancy
over time graph, grouped (or faceted) by continent. We will remove all
legends, adding the `theme(legend.position="none")` in the end of our
ggplot.

```{r lifeExp_facet_by_continent}
ggplot(data = gapminder , mapping = aes(x = year, y = lifeExp, colour= country))+
  geom_point() + 
  geom_smooth(se = FALSE) +
  facet_wrap(~continent) +
  theme(legend.position="none") + #remove all legends
  NULL
```

Given these trends, what can you say about life expectancy since 1952?
Again, don't just say what's happening in the graph. Tell some sort of
story and speculate about the differences in the patterns.

> Type your answer after this blockquote. 

With only minor exemptions (e.g. bump in Europe in the 80s, probably Baltic states), it is visible that life expectancy in the Americas, Europe and Oceania has increased fairly steadily. This data supports the common believe, that advancements in medicine and hygiene, stable nutrition supply, less wars and overall enhancements in quality of life have contributed to an increase in life expectancy.\

In Asia and Africa, however, there is once again an overall trend to increasing life expectancy but fluctuations and exemptions are more common. Especially in some African countries we see a sharp decline in life expectancy starting from the 80s and 90s. It can be speculated that this is due to epidemics - especially HIV â€“ but also due to civil wars or famines. The average life expectancy in Asia and Africa reached in 2007 is well below that of Europe and Oceania. The standard deviation is also higher, which leads to the conclusion that there are major difference among single countries.


# Task 3: Brexit vote analysis

We will have a look at the results of the 2016 Brexit vote in the UK.
First we read the data using `read_csv()` and have a quick glimpse at
the data

```{r load_brexit_data, warning=FALSE, message=FALSE}

# read data directly off github repo
brexit_results <- read_csv("https://raw.githubusercontent.com/kostis-christodoulou/am01/master/data/brexit_results.csv")


glimpse(brexit_results)
```

The data comes from [Elliott Morris](https://www.thecrosstab.com/), who
cleaned it and made it available through his [DataCamp class on
analysing election and polling data in
R](https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r).

Our main outcome variable (or y) is `leave_share`, which is the percent
of votes cast in favour of Brexit, or leaving the EU. Each row is a UK
[parliament
constituency](https://en.wikipedia.org/wiki/United_Kingdom_Parliament_constituencies).

To get a sense of the spread, or distribution, of the data, we can plot
a histogram, a density plot, and the empirical cumulative distribution
function of the leave % in all constituencies.

```{r brexit_histogram, warning=FALSE, message=FALSE}

# histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5) +
  labs(title = "Histogram of leave % during Brexit Vote",
       subtitle = "All constituencies",
       x = "leave %-share",
       y = "# constituencies")

# density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density() + 
  labs(title = "Density plot of leave % during Brexit Vote",
       subtitle = "All constituencies",
       x = "leave %-share",
       y = "Constituencies density")


# The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Cumulative Distribution of leave % during Brexit Vote",
       subtitle = "All constituencies",
       x = "leave %-share",
       y = "Cumulated share of constituencies")
  
```

One common explanation for the Brexit outcome was fear of immigration
and opposition to the EU's more open border policy. We can check the
relationship (or correlation) between the proportion of native born
residents (`born_in_uk`) in a constituency and its `leave_share`. To do
this, let us get the correlation between the two variables

```{r brexit_immigration_correlation}
brexit_results %>% 
  select(leave_share, born_in_uk) %>% 
  cor()
```

The correlation is almost 0.5, which shows that the two variables are
positively correlated.

We can also create a scatterplot between these two variables using
`geom_point`. We also add the best fit line, using
`geom_smooth(method = "lm")`.

```{r brexit_immigration_plot}
ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  
  # add a smoothing line, and use method="lm" to get the best straight-line
  geom_smooth(method = "lm") + 
  
  # use a white background and frame the plot with a black box
  theme_bw() +
  
  # Add titles
  labs(title = "Relationship between native borne residents and leave share",
       subtitle = "All constituencies",
       x = "native borne uk residents %-share",
       y = "leave %-share") +
  NULL
```

You have the code for the plots, I would like you to revisit all of them
and use the `labs()` function to add an informative title, subtitle, and
axes titles to all plots.

What can you say about the relationship shown above? Again, don't just
say what's happening in the graph. Tell some sort of story and speculate
about the differences in the patterns.

> Type your answer after, and outside, this blockquote.

The graph shows a correlation (0.5) between the % of native born residents within a constituency and its leave %-share. A large bulk of constituencies has a born_in_uk rate of 90% and above whereby the average of leave %-share among this majority-group seems to lie around 55%. In constituencies where the % of native born residents is below 80% on the other hand, the average submitted votes would have kept the UK within the EU. These findings support the statement that patriotic, UK born citizens are more likely to oppose the EU and its regulations (among which looser immigration policies). \

However, a significant standard deviation can be observed across all %-shares of native borne uk residents. In other words, this means that two constituencies with a similiar 80% "native share" have voted in favor of Brexit with 70% as well as 20%. Thus, it is suggested not to regard native born % and fear of immigration as only explanation for the Brexit outcome.

# Task 4: Animal rescue incidents attended by the London Fire Brigade

[The London Fire
Brigade](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb)
attends a range of non-fire incidents (which we call 'special
services'). These 'special services' include assistance to animals that
may be trapped or in distress. The data is provided from January 2009
and is updated monthly. A range of information is supplied for each
incident including some location information (postcode, borough, ward),
as well as the data/time of the incidents. We do not routinely record
data about animal deaths or injuries.

Please note that any cost included is a notional cost calculated based
on the length of time rounded up to the nearest hour spent by Pump,
Aerial and FRU appliances at the incident and charged at the current
Brigade hourly rate.

```{r load_animal_rescue_data, warning=FALSE, message=FALSE}

url <- "https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/f43b485e-fb35-419c-aa7a-fa75676e5835/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv"


animal_rescue <- read_csv(url,
                          locale = locale(encoding = "CP1252")) %>% 
  janitor::clean_names()


glimpse(animal_rescue)
```

One of the more useful things one can do with any data set is quick
counts, namely to see how many observations fall within one category.
For instance, if we wanted to count the number of incidents by year, we
would either use `group_by()... summarise()` or, simply
[`count()`](https://dplyr.tidyverse.org/reference/count.html)

```{r, instances_by_calendar_year}

animal_rescue %>% 
  dplyr::group_by(cal_year) %>% 
  summarise(count=n())

animal_rescue %>% 
  count(cal_year, name="count")

```

Let us try to see how many incidents we have by animal group. Again, we
can do this either using group_by() and summarise(), or by using count()

```{r, animal_group_percentages}
animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  
  #group_by and summarise will produce a new column with the count in each animal group
  summarise(count = n()) %>% 
  
  # mutate adds a new column; here we calculate the percentage
  mutate(percent = round(100*count/sum(count),2)) %>% 
  
  # arrange() sorts the data by percent. Since the default sorting is min to max and we would like to see it sorted
  # in descending order (max to min), we use arrange(desc()) 
  arrange(desc(percent))


animal_rescue %>% 
  
  #count does the same thing as group_by and summarise
  # name = "count" will call the column with the counts "count" ( exciting, I know)
  # and 'sort=TRUE' will sort them from max to min
  count(animal_group_parent, name="count", sort=TRUE) %>% 
  mutate(percent = round(100*count/sum(count),2))


```

Do you see anything strange in these tables?\

Cat/cat appears two times, some animals have not been categorized, top 5 categories combined over 90% of incidents.

Finally, let us have a loot at the notional cost for rescuing each of
these animals. As the LFB says,

> Please note that any cost included is a notional cost calculated based
> on the length of time rounded up to the nearest hour spent by Pump,
> Aerial and FRU appliances at the incident and charged at the current
> Brigade hourly rate.

There is two things we will do:

1.  Calculate the mean and median `incident_notional_cost` for each
    `animal_group_parent`
2.  Plot a boxplot to get a feel for the distribution of
    `incident_notional_cost` by `animal_group_parent`.

Before we go on, however, we need to fix `incident_notional_cost` as it
is stored as a `chr`, or character, rather than a number.

```{r, parse_incident_cost,message=FALSE, warning=FALSE}

# what type is variable incident_notional_cost from dataframe `animal_rescue`
typeof(animal_rescue$incident_notional_cost)

# readr::parse_number() will convert any numerical values stored as characters into numbers
animal_rescue <- animal_rescue %>% 

  # we use mutate() to use the parse_number() function and overwrite the same variable
  mutate(incident_notional_cost = parse_number(incident_notional_cost))

# incident_notional_cost from dataframe `animal_rescue` is now 'double' or numeric
typeof(animal_rescue$incident_notional_cost)

```

Now that incident_notional_cost is numeric, let us quickly calculate
summary statistics for each animal group.

```{r, stats_on_incident_cost,message=FALSE, warning=FALSE}

animal_rescue %>% 
  
  # group by animal_group_parent
  group_by(animal_group_parent) %>% 
  
  # filter resulting data, so each group has at least 6 observations
  filter(n()>6) %>% 
  
  # summarise() will collapse all values into 3 values: the mean, median, and count  
  # we use na.rm=TRUE to make sure we remove any NAs, or cases where we do not have the incident cos
  summarise(mean_incident_cost = mean (incident_notional_cost, na.rm=TRUE),
            median_incident_cost = median (incident_notional_cost, na.rm=TRUE),
            sd_incident_cost = sd (incident_notional_cost, na.rm=TRUE),
            min_incident_cost = min (incident_notional_cost, na.rm=TRUE),
            max_incident_cost = max (incident_notional_cost, na.rm=TRUE),
            count = n()) %>% 
  
  # sort the resulting data in descending order. You choose whether to sort by count or mean cost.
  arrange(desc(mean_incident_cost))

```

Compare the mean and the median for each animal group. waht do you think
this is telling us? Anything else that stands out? Any outliers?\

> Response

Median usually lower, this lets us conclude that we have outliers on the top end of the cost range (single animals very difficult to rescue). 

Finally, let us plot a few plots that show the distribution of
incident_cost for each animal group.

```{r, plots_on_incident_cost_by_animal_group,message=FALSE, warning=FALSE}

# base_plot
base_plot <- animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = "free")+
  theme_bw()

base_plot + geom_histogram()
base_plot + geom_density()
base_plot + geom_boxplot()
base_plot + stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)



```

Which of these four graphs do you think best communicates the
variability of the `incident_notional_cost` values? Also, can you please
tell some sort of story (which animals are more expensive to rescue than
others, the spread of values) and speculate about the differences in the
patterns.\

> Response

Variability is best shown in the boxplot graph as single outliers get lost in the other graphs due to the significant number of observations.\ 
As expected, large animals such as horses, cows, and deer are on average the most expensive to rescue. It is speculated that it takes more working-hours and machinery to recover heavier animals. While the minimal cost is similiar across the categories, the maximum cost seams to have some positive correlation with animal size.\ 
Correspondingly, the sd is significant in these categories, with the maximum value as high as 10x the median rescue cost. Animals such as Rabbits and Ferrets have the least outliers, a fact that could lead to the conclusion that the rescue effort for these animals is fairly predictable.


# Submit the assignment

Knit the completed R Markdown file as an HTML document (use the "Knit"
button at the top of the script editor window) and upload it to Canvas.

## Details

If you want to, please answer the following

-   Who did you collaborate with: *none*
-   Approximately how much time did you spend on this problem set: *~ 2h*
-   What, if anything, gave you the most trouble: *write a meaningful explanation of the graphs (e.g. deciding which patterns are relevant and should thus be highlighted)*
